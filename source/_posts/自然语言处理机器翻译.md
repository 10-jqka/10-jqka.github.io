---
title: 自然语言处理机器翻译
date: 2024-06-25 23:19:38
categories: [2024夏]
tags: [人工智能, 自然语言处理]
excerpt: 基于pytorch，实现简单的机器翻译。
thumbnail: /images/blog/nlp_machinetrans/废物.JPG
---

{% notel blue 'fa-solid fa-book' '前言' %}
手机欠费了...好崩溃...
{% endnotel %}

- [概要](#概要)
- [内容](#内容)
  - [编码器-解码器](#编码器-解码器)
  - [搜索](#搜索)
    - [贪婪搜索](#贪婪搜索)
    - [束搜索](#束搜索)
  - [注意力机制](#注意力机制)
  - [机器翻译](#机器翻译)
- [总结](#总结)

## 概要

1. 介绍编码器-解码器、束搜索、注意力机制、机器翻译。
2. 根据上述内容，进行机器翻译实践。

## 内容

### 编码器-解码器

编码器-解码器用于解决不定长序列问题。编码器用来分析输入序列，解码器用来生成输出序列。

![编码器解码器](/source/images/blog/nlp_machinetrans/10.9_seq2seq.svg)

编码器-解码器由两部分循环神经网络组成，编码器将各个时间步的隐状态转换为背景变量，而解码器根据背景变量建模当前时间步输出的概率分布。

对模型训练，我们使用最大似然估计，模型loss为：

$$
-\log P(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) = -\sum_{t'=1}^{T'} \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}),
$$

在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。我们需要将解码器在上一个时间步的输出作为当前时间步的输入。或者，在训练中我们也可以将标签序列（训练集的真实输出序列）在上一个时间步的标签作为解码器在当前时间步的输入。这叫作强制教学（teacher forcing）。

### 搜索

模型训练好了之后，我们还要使用模型得到预测结果。

在上一节中，我们介绍了编码器解码器模型，假设模型训练完毕，输入一个句子，我们得到的是一堆概率分布。具体的，是所有可能的输出序列及其概率值。

#### 贪婪搜索

如图，假设模型得到了这样一堆概率分布作为输出，贪婪搜索在当前时间步中选择概率最大的词，并进行下一步的搜索。

![prob1](/source/images/blog/nlp_machinetrans/10.10_s2s_prob1.svg)

然而，贪婪搜索并不能保证全局最优解，事实上，它很容易陷入局部最优。

![prob2](/source/images/blog/nlp_machinetrans/10.10_s2s_prob2.svg)

#### 束搜索

显然，我们希望搜索到最大概率的序列，但是穷举搜索整个概率分布是不现实的。为了尽可能得到全局最优解，我们使用束搜索。

![束搜索](/source/images/blog/nlp_machinetrans/10.10_beam_search.svg)

束搜索（beam search）是对贪婪搜索的一个改进算法。它有一个束宽（beam size）超参数。我们将它设为$k$。在时间步1时，选取当前时间步条件概率最大的$k$个词，分别组成$k$个候选输出序列的首词。在之后的每个时间步，基于上个时间步的$k$个候选输出序列，从$k\left|\mathcal{Y}\right|$个可能的输出序列中选取条件概率最大的$k$个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt;eos&gt;”的序列，并将它们中所有特殊符号“&lt;eos&gt;”后面的子序列舍弃，得到最终候选输出序列的集合。

可以近似的理解为，束搜索是一种更宽的贪婪搜索————它不仅只看最好的，还看第二好的，第三好的......并在这基础上继续往下搜索。当然，普通的贪婪搜索也可以视为一种束宽为1的束搜索。

### 注意力机制

注意力机制是一种在深度学习中广泛应用的技术，其核心思想是通过关注输入数据中的特定部分来提高模型的性能和效率。

### 机器翻译

## 总结
