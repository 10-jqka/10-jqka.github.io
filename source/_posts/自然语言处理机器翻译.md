---
title: 自然语言处理机器翻译
date: 2024-06-25 23:19:38
categories: [2024夏]
tags: [人工智能, 自然语言处理]
excerpt: 基于pytorch，实现简单的机器翻译。
thumbnail: /images/blog/nlp_machinetrans/废物.JPG
---

{% notel blue 'fa-solid fa-book' '前言' %}
手机欠费了...好崩溃...
{% endnotel %}

- [概要](#概要)
- [内容](#内容)
  - [编码器-解码器](#编码器-解码器)
  - [搜索](#搜索)
    - [贪婪搜索](#贪婪搜索)
    - [束搜索](#束搜索)
  - [注意力机制](#注意力机制)
    - [计算](#计算)
  - [机器翻译](#机器翻译)
    - [数据处理](#数据处理)
    - [模型实现](#模型实现)
      - [编码器](#编码器)
      - [解码器](#解码器)
    - [训练评估](#训练评估)
- [总结](#总结)

## 概要

1. 介绍编码器-解码器、束搜索、注意力机制、机器翻译。
2. 根据上述内容，进行机器翻译实践。

## 内容

### 编码器-解码器

编码器-解码器用于解决不定长序列问题。编码器用来分析输入序列，解码器用来生成输出序列。

![编码器解码器](/source/images/blog/nlp_machinetrans/10.9_seq2seq.svg)

编码器-解码器由两部分循环神经网络组成，编码器将各个时间步的隐状态转换为背景变量，而解码器根据背景变量建模当前时间步输出的概率分布。

对模型训练，我们使用最大似然估计，模型loss为：

$$
-\log P(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) = -\sum_{t'=1}^{T'} \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}),
$$

在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。我们需要将解码器在上一个时间步的输出作为当前时间步的输入。或者，在训练中我们也可以将标签序列（训练集的真实输出序列）在上一个时间步的标签作为解码器在当前时间步的输入。这叫作强制教学（teacher forcing）。

### 搜索

模型训练好了之后，我们还要使用模型得到预测结果。

在上一节中，我们介绍了编码器解码器模型，假设模型训练完毕，输入一个句子，我们得到的是一堆概率分布。具体的，是所有可能的输出序列及其概率值。

#### 贪婪搜索

如图，假设模型得到了这样一堆概率分布作为输出，贪婪搜索在当前时间步中选择概率最大的词，并进行下一步的搜索。

![prob1](/source/images/blog/nlp_machinetrans/10.10_s2s_prob1.svg)

然而，贪婪搜索并不能保证全局最优解，事实上，它很容易陷入局部最优。

![prob2](/source/images/blog/nlp_machinetrans/10.10_s2s_prob2.svg)

#### 束搜索

显然，我们希望搜索到最大概率的序列，但是穷举搜索整个概率分布是不现实的。为了尽可能得到全局最优解，我们使用束搜索。

![束搜索](/source/images/blog/nlp_machinetrans/10.10_beam_search.svg)

束搜索（beam search）是对贪婪搜索的一个改进算法。它有一个束宽（beam size）超参数。我们将它设为$k$。在时间步1时，选取当前时间步条件概率最大的$k$个词，分别组成$k$个候选输出序列的首词。在之后的每个时间步，基于上个时间步的$k$个候选输出序列，从$k\left|\mathcal{Y}\right|$个可能的输出序列中选取条件概率最大的$k$个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt;eos&gt;”的序列，并将它们中所有特殊符号“&lt;eos&gt;”后面的子序列舍弃，得到最终候选输出序列的集合。

可以近似的理解为，束搜索是一种更宽的贪婪搜索————它不仅只看最好的，还看第二好的，第三好的......并在这基础上继续往下搜索。当然，普通的贪婪搜索也可以视为一种束宽为1的束搜索。

### 注意力机制

注意力机制是一种在深度学习中广泛应用的技术，其核心思想是通过关注输入数据中的特定部分来提高模型的性能和效率。

#### 计算

具体来说，令编码器在时间步$t$的隐藏状态为$\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t'$的背景变量为所有编码器隐藏状态的加权平均：

$$
\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,
$$

其中给定$t'$时，权重$\alpha_{t' t}$在$t=1,\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:

$$
\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.
$$

现在，我们需要定义如何计算上式中softmax运算的输入$e_{t' t}$。由于$e_{t' t}$同时取决于解码器的时间步$t'$和编码器的时间步$t$，我们不妨以解码器在时间步$t'-1$的隐藏状态$\boldsymbol{s}_{t' - 1}$与编码器在时间步$t$的隐藏状态$\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t' t}$：

$$
e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).
$$

这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]：

$$
a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),
$$

其中$\boldsymbol{v}$、$\boldsymbol{W}_s$、$\boldsymbol{W}_h$都是可以学习的模型参数。

我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。

在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。
让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为$h$，且函数$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。假设我们希望根据解码器单个隐藏状态$\boldsymbol{s}_{t' - 1} \in \mathbb{R}^{h}$和编码器所有隐藏状态$\boldsymbol{h}_t \in \mathbb{R}^{h}, t = 1,\ldots,T$来计算背景向量$\boldsymbol{c}_{t'}\in \mathbb{R}^{h}$。
我们可以将查询项矩阵$\boldsymbol{Q} \in \mathbb{R}^{1 \times h}$设为$\boldsymbol{s}_{t' - 1}^\top$，并令键项矩阵$\boldsymbol{K} \in \mathbb{R}^{T \times h}$和值项矩阵$\boldsymbol{V} \in \mathbb{R}^{T \times h}$相同且第$t$行均为$\boldsymbol{h}_t^\top$。此时，我们只需要通过矢量化计算

$$\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}$$

即可算出转置后的背景向量$\boldsymbol{c}_{t'}^\top$。当查询项矩阵$\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。

### 机器翻译

#### 数据处理

我们先定义一些特殊符号。其中“&lt;pad&gt;”（padding）符号用来添加在较短序列后，直到每个序列等长，而“&lt;bos&gt;”和“&lt;eos&gt;”符号分别表示序列的开始和结束。

```python
import collections
import os
import io
import math
import torch
from torch import nn
import torch.nn.functional as F
import torchtext.vocab as Vocab
import torch.utils.data as Data

import sys
# sys.path.append("..") 
import d2lzh_pytorch as d2l

# 定义常量PAD、BOS、EOS，分别表示填充、开始和结束
PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'
# 设置环境变量，指定使用的GPU设备编号
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
# 根据当前设备是否支持CUDA，选择使用CPU或GPU进行计算
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 打印PyTorch版本和使用的计算设备
print(torch.__version__, device)
```

定义两个辅助函数对后面读取的数据进行预处理。

```python
# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列
# 长度变为max_seq_len，然后将序列保存在all_seqs中
def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):
    all_tokens.extend(seq_tokens)
    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)
    all_seqs.append(seq_tokens)

# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor
def build_data(all_tokens, all_seqs):
    vocab = Vocab.Vocab(collections.Counter(all_tokens),
                        specials=[PAD, BOS, EOS])
    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs]
    return vocab, torch.tensor(indices)
```

在这个数据集里，每一行是一对法语句子和它对应的英语句子，中间使用`'\t'`隔开。在读取数据时，我们在句末附上“&lt;eos&gt;”符号，并可能通过添加“&lt;pad&gt;”符号使每个序列的长度均为`max_seq_len`。法语词的索引和英语词的索引相互独立。

```python
def read_data(max_seq_len):
    # 初始化输入和输出的tokens和序列列表
    in_tokens, out_tokens, in_seqs, out_seqs = [], [], [], []
    # 读取文件内容
    with io.open('fr-en-small.txt') as f:
        lines = f.readlines()
    
    # 遍历每一行
    for line in lines:
        # 去除行尾空格，并以制表符分割输入和输出序列
        in_seq, out_seq = line.rstrip().split('\t')
        # 将输入和输出序列按空格分割为tokens
        in_seq_tokens, out_seq_tokens = in_seq.split(' '), out_seq.split(' ')
        # 如果加上EOS后长于max_seq_len，则忽略掉此样本
        if max(len(in_seq_tokens), len(out_seq_tokens)) > max_seq_len - 1:
            continue
        # 处理输入序列
        process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)
        # 处理输出序列
        process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)
    # 构建输入数据
    in_vocab, in_data = build_data(in_tokens, in_seqs)
    # 构建输出数据
    out_vocab, out_data = build_data(out_tokens, out_seqs)    
    # 返回输入和输出的词汇表以及数据集
    return in_vocab, out_vocab, Data.TensorDataset(in_data, out_data)
```

将序列的最大长度设成7，然后查看读取到的第一个样本。该样本分别包含法语词索引序列和英语词索引序列。

```python
max_seq_len = 7
in_vocab, out_vocab, dataset = read_data(max_seq_len)
dataset[0]
```

>(tensor([ 5,  4, 45,  3,  2,  0,  0]), tensor([ 8,  4, 27,  3,  2,  0,  0]))

#### 模型实现

##### 编码器

```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 drop_prob=0, **kwargs):
        # 初始化Encoder类，继承自nn.Module
        super(Encoder, self).__init__(**kwargs)
        # 定义嵌入层，将词汇表大小映射到嵌入维度
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # 定义GRU层，输入为嵌入维度，隐藏层大小为num_hiddens，层数为num_layers，dropout概率为drop_prob
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=drop_prob)


    def forward(self, inputs, state):
        # 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维
        embedding = self.embedding(inputs.long()).permute(1, 0, 2) # (seq_len, batch, input_size)
        return self.rnn(embedding, state)

    def begin_state(self):
        return None
```

##### 解码器

```python
def attention_model(input_size, attention_size):
    model = nn.Sequential(nn.Linear(input_size, attention_size, bias=False),
                          nn.Tanh(),
                          nn.Linear(attention_size, 1, bias=False))
    return model

def attention_forward(model, enc_states, dec_state):
    """
    enc_states: (时间步数, 批量大小, 隐藏单元个数)
    dec_state: (批量大小, 隐藏单元个数)
    """
    # 将解码器隐藏状态广播到和编码器隐藏状态形状相同后进行连结
    dec_states = dec_state.unsqueeze(dim=0).expand_as(enc_states)
    enc_and_dec_states = torch.cat((enc_states, dec_states), dim=2)
    e = model(enc_and_dec_states)  # 形状为(时间步数, 批量大小, 1)
    alpha = F.softmax(e, dim=0)  # 在时间步维度做softmax运算
    return (alpha * enc_states).sum(dim=0)  # 返回背景变量
```

```python
class Decoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 attention_size, drop_prob=0):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.attention = attention_model(2*num_hiddens, attention_size)
        # GRU的输入包含attention输出的c和实际输入, 所以尺寸是 num_hiddens+embed_size
        self.rnn = nn.GRU(num_hiddens + embed_size, num_hiddens, 
                          num_layers, dropout=drop_prob)
        self.out = nn.Linear(num_hiddens, vocab_size)

    def forward(self, cur_input, state, enc_states):
        """
        cur_input shape: (batch, )
        state shape: (num_layers, batch, num_hiddens)
        """
        # 使用注意力机制计算背景向量
        c = attention_forward(self.attention, enc_states, state[-1])
        # 将嵌入后的输入和背景向量在特征维连结, (批量大小, num_hiddens+embed_size)
        input_and_c = torch.cat((self.embedding(cur_input), c), dim=1) 
        # 为输入和背景向量的连结增加时间步维，时间步个数为1
        output, state = self.rnn(input_and_c.unsqueeze(0), state)
        # 移除时间步维，输出形状为(批量大小, 输出词典大小)
        output = self.out(output).squeeze(dim=0)
        return output, state

    def begin_state(self, enc_state):
        # 直接将编码器最终时间步的隐藏状态作为解码器的初始隐藏状态
        return enc_state
```

#### 训练评估

## 总结

无。
